{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTMTagger.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4JeBFpyx3FP",
        "outputId": "edcdea99-fffb-460a-b214-f3d50eef5ce1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9hK8BXux5OR"
      },
      "source": [
        "# !pip install conllu\n",
        "\n",
        "from io import open\n",
        "from conllu import parse_incr\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "\n",
        "data_file = open(\"/content/drive/My Drive/zh_gsd-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
        "\n",
        "# save dataset\n",
        "sentences_list = []\n",
        "pos_list = []\n",
        "\n",
        "for tokenlist in parse_incr(data_file):\n",
        "    temp_str = []\n",
        "    temp_pos = []\n",
        "    for s in tokenlist:\n",
        "        temp_str.append(s['form'])\n",
        "        temp_pos.append(s['upos'])\n",
        "    sentences_list.append(temp_str)\n",
        "    pos_list.append(temp_pos)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fj9LbD4u949"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRm0UvB3yMz3"
      },
      "source": [
        "train_sentence = sentences_list[:3500]\n",
        "train_pos = pos_list[:3500]\n",
        "\n",
        "test_sentence = sentences_list[3500:]\n",
        "test_pos = pos_list[3500:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbilCw-VLRZo"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# Train Word2Vec model\n",
        "emb_model = Word2Vec(sentences_list, size=250, window=5, min_count=1, workers=4, sg=1, hs=1, iter=50)\n",
        "num_words = len(emb_model.wv.vocab) + 1\n",
        "pad = '<pad>'\n",
        "unk = '<unk>'\n",
        "pad_id = 0\n",
        "unk_id = 1\n",
        "\n",
        "idx2word = {\n",
        "  0: '<pad>',\n",
        "}\n",
        "\n",
        "word2idx = {\n",
        "  '<pad>': 0,\n",
        "}\n",
        "\n",
        "count_idx2w = 1\n",
        "count_w2idx = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edmXk466_lRF"
      },
      "source": [
        "emb_matrix = np.zeros((num_words, 250), dtype=float)\n",
        "for i in range(num_words - 1):\n",
        "    v = emb_model.wv[emb_model.wv.index2word[i]]\n",
        "    emb_matrix[i+1] = v   # Plus 1 to reserve index 0 for OOV words\n",
        "    idx2word[count_idx2w] = emb_model.wv.index2word[i]\n",
        "    word2idx[emb_model.wv.index2word[i]] = count_w2idx\n",
        "    count_idx2w += 1\n",
        "    count_w2idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYED5c37FUcn",
        "outputId": "bcd17c19-8254-4f2f-cbe6-8691a7d0042c"
      },
      "source": [
        "emb_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17613, 250)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZPwURTCSnDB"
      },
      "source": [
        "assert len(idx2word) == len(word2idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-b4pQonya2N"
      },
      "source": [
        "def add_dict(list_data: list, first_insert_list: list):\n",
        "    dict_data = {}\n",
        "    idx2dict = {}\n",
        "    count = 0\n",
        "\n",
        "    if len(first_insert_list) != 0:\n",
        "      for ele in first_insert_list:\n",
        "        if ele not in dict_data:\n",
        "          dict_data[ele] = count\n",
        "          idx2dict[count] = ele\n",
        "          count += 1\n",
        "    \n",
        "    for first_ele in list_data:\n",
        "        for second_ele in first_ele:\n",
        "            if second_ele not in dict_data:\n",
        "                dict_data[second_ele] = count\n",
        "                idx2dict[count] = second_ele\n",
        "                count += 1\n",
        "    \n",
        "    return dict_data, idx2dict\n",
        "\n",
        "# 建立辭典\n",
        "# dict_first_insert_word = [pad, unk]\n",
        "\n",
        "# dict_words, idx2word = add_dict(sentences_list, [])\n",
        "dict_pos, idx2pos = add_dict(pos_list, ['igner'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxE87i9Oklln",
        "outputId": "302aa7dd-82e3-4096-c010-d085fc217927"
      },
      "source": [
        "dict_pos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ADJ': 2,\n",
              " 'ADP': 10,\n",
              " 'ADV': 4,\n",
              " 'AUX': 1,\n",
              " 'CCONJ': 12,\n",
              " 'DET': 11,\n",
              " 'NOUN': 7,\n",
              " 'NUM': 6,\n",
              " 'PART': 9,\n",
              " 'PRON': 8,\n",
              " 'PROPN': 13,\n",
              " 'PUNCT': 3,\n",
              " 'SYM': 15,\n",
              " 'VERB': 5,\n",
              " 'X': 14,\n",
              " 'igner': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g2T8S3KyeaL"
      },
      "source": [
        "# 建立數據集，餵給LSTM使用\n",
        "class POS_Dataset(Dataset):\n",
        "    def __init__(self, sentence_list, pos_list, word2idx, dict_pos):\n",
        "        self.sentence_list = sentence_list\n",
        "        self.pos_list = pos_list\n",
        "        self.dict_words = word2idx\n",
        "        self.dict_pos = dict_pos\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        source = self.sentence_list[idx]\n",
        "        target = self.pos_list[idx]\n",
        "\n",
        "        data_X = [self.dict_words[w] for w in source]\n",
        "        target_Y = [self.dict_pos[tag] for tag in target]\n",
        "        \n",
        "        data_X = torch.tensor(data_X, dtype=torch.long)\n",
        "        target_Y = torch.tensor(target_Y, dtype=torch.long)\n",
        "        \n",
        "        return data_X, target_Y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.sentence_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDjmECzHygD5"
      },
      "source": [
        "def pad_collate(batch):\n",
        "  (xx, yy) = zip(*batch)\n",
        "  x_lens = [len(x) for x in xx]\n",
        "  y_lens = [len(y) for y in yy]\n",
        "\n",
        "  xx_pad = pad_sequence(xx, batch_first=True, padding_value=pad_id)\n",
        "  yy_pad = pad_sequence(yy, batch_first=True, padding_value=dict_pos['igner'])\n",
        "\n",
        "  return xx_pad, yy_pad, x_lens, y_lens\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "dataset = POS_Dataset(train_sentence, train_pos, word2idx, dict_pos)\n",
        "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
        "\n",
        "test_dataset = POS_Dataset(test_sentence, test_pos, word2idx, dict_pos)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=len(sentences_list)-3500, shuffle=False, collate_fn=pad_collate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUwahHWpyiV8"
      },
      "source": [
        "# 建立模型 inputs > nn.Embedding > nn.LSTM > nn.Dropout > 取最後一個 state > nn.Linear > softmax\n",
        "class LSTM_Tagger(nn.Module):\n",
        "    def __init__(self, n_hidden, num_layers, dropout_ratio, pos_size, weight):\n",
        "        super(LSTM_Tagger, self).__init__()\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "        self.word_embeddings = nn.Embedding.from_pretrained(weight, padding_idx = 0)\n",
        "        self.word_embeddings.weight.requires_grad=False\n",
        "\n",
        "        self.pos_size = pos_size\n",
        "        self.lstm = nn.LSTM(input_size=250, hidden_size=n_hidden, dropout=dropout_ratio, num_layers=num_layers, bidirectional=True)\n",
        "        self.fc1 = nn.Linear(in_features=n_hidden*2, out_features=128)\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=pos_size)\n",
        "\n",
        "\n",
        "    def forward(self, inputs, x_len):\n",
        "        batch_size = inputs.size(0)\n",
        "        embedded = self.word_embeddings(inputs)\n",
        "\n",
        "        x_packed = pack_padded_sequence(embedded, x_len, batch_first=True, enforce_sorted=False)\n",
        "        \n",
        "        output_packed, (hidden, cell) = self.lstm(x_packed)\n",
        "        outputs, output_lengths = pad_packed_sequence(output_packed, batch_first=True)\n",
        "\n",
        "        logits = self.fc1(outputs)\n",
        "        logits = self.dropout(logits)\n",
        "        logits = self.fc2(logits).view(-1, self.pos_size)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXLtuK8VN6m2"
      },
      "source": [
        "def categorical_accuracy(preds, y, tag_pad_idx):\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
        "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
        "    return correct.sum() / y[non_pad_elements].shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdHVftLMykis"
      },
      "source": [
        "def train_batch(model, x_padded, y_padded, x_lens, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    inputs, targets = x_padded.to(device), y_padded.to(device)\n",
        "    outputs = model(inputs, x_lens)\n",
        "    targets = targets.view(-1)\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    acc = categorical_accuracy(outputs, targets, 0)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    return loss.item(), acc.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4LGOW0xymxF",
        "outputId": "9e3482d6-5f73-460d-9438-066841194a59"
      },
      "source": [
        "# 訓練模型\n",
        "epochs = 100\n",
        "lr = 0.0001\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = LSTM_Tagger(128, 1, 0.8, len(dict_pos), torch.Tensor(emb_matrix))\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
        "criterion.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "for epoch in range(1, 1 + epochs):\n",
        "    tot_train_loss = 0\n",
        "    tot_train_acc = 0\n",
        "    tot_val_loss = 0\n",
        "    tot_val_acc = 0\n",
        "\n",
        "    for (x_padded, y_padded, x_lens, y_lens) in train_loader:\n",
        "      loss, acc = train_batch(model, x_padded, y_padded, x_lens, criterion, optimizer, device)\n",
        "      tot_train_loss += loss\n",
        "      tot_train_acc += acc\n",
        "\n",
        "    train_loss_list.append(tot_train_loss/35)\n",
        "    \n",
        "    \n",
        "    model.eval()\n",
        "    for (x_padded, y_padded, x_lens, y_lens) in test_loader:\n",
        "      inputs, targets = x_padded.to(device), y_padded.to(device)\n",
        "      outputs = model(inputs, x_lens)\n",
        "      targets = targets.view(-1)\n",
        "      loss = criterion(outputs, targets).item()\n",
        "      acc = categorical_accuracy(outputs, targets, 0).item()\n",
        "      tot_val_loss += loss\n",
        "      tot_val_acc += acc\n",
        "\n",
        "    val_loss_list.append(tot_val_loss)\n",
        "\n",
        "    print('epoch ', epoch, 'train_loss: ', tot_train_loss/35, '| train_acc: ', tot_train_acc/35)\n",
        "    print('       ', 'val_loss: ', tot_val_loss, '| val_acc: ', tot_val_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  1 train_loss:  2.695020866394043 | train_acc:  0.16741396486759186\n",
            "        val_loss:  2.5978410243988037 | val_acc:  0.26635661721229553\n",
            "epoch  2 train_loss:  2.457591574532645 | train_acc:  0.2616861245461873\n",
            "        val_loss:  2.279318332672119 | val_acc:  0.2661989629268646\n",
            "epoch  3 train_loss:  2.29945205279759 | train_acc:  0.2611251200948443\n",
            "        val_loss:  2.205702781677246 | val_acc:  0.2661989629268646\n",
            "epoch  4 train_loss:  2.230020877293178 | train_acc:  0.2853335993630545\n",
            "        val_loss:  2.1426844596862793 | val_acc:  0.306952565908432\n",
            "epoch  5 train_loss:  2.156456654412406 | train_acc:  0.3150904629911695\n",
            "        val_loss:  2.0579771995544434 | val_acc:  0.3492826819419861\n",
            "epoch  6 train_loss:  2.0568279879433766 | train_acc:  0.3557708850928715\n",
            "        val_loss:  1.9391855001449585 | val_acc:  0.3889327049255371\n",
            "epoch  7 train_loss:  1.9328834635870797 | train_acc:  0.39827234404427664\n",
            "        val_loss:  1.7977041006088257 | val_acc:  0.42054232954978943\n",
            "epoch  8 train_loss:  1.8039586237498693 | train_acc:  0.42733543515205386\n",
            "        val_loss:  1.6679707765579224 | val_acc:  0.4460034668445587\n",
            "epoch  9 train_loss:  1.6970640557152885 | train_acc:  0.45084882719176156\n",
            "        val_loss:  1.5623979568481445 | val_acc:  0.4736717641353607\n",
            "epoch  10 train_loss:  1.6094292197908675 | train_acc:  0.4773021766117641\n",
            "        val_loss:  1.4747511148452759 | val_acc:  0.5114299058914185\n",
            "epoch  11 train_loss:  1.5308233635766166 | train_acc:  0.5048311386789595\n",
            "        val_loss:  1.3990898132324219 | val_acc:  0.5441431403160095\n",
            "epoch  12 train_loss:  1.4642313514436995 | train_acc:  0.5298753789493016\n",
            "        val_loss:  1.3320190906524658 | val_acc:  0.567318320274353\n",
            "epoch  13 train_loss:  1.3994926861354282 | train_acc:  0.5557740569114685\n",
            "        val_loss:  1.2733287811279297 | val_acc:  0.5912817716598511\n",
            "epoch  14 train_loss:  1.3461921317236765 | train_acc:  0.5748182773590088\n",
            "        val_loss:  1.217423439025879 | val_acc:  0.6115403175354004\n",
            "epoch  15 train_loss:  1.2968356268746513 | train_acc:  0.5930642145020621\n",
            "        val_loss:  1.1683648824691772 | val_acc:  0.6316412091255188\n",
            "epoch  16 train_loss:  1.251688323702131 | train_acc:  0.6081166982650756\n",
            "        val_loss:  1.1268895864486694 | val_acc:  0.6506385207176208\n",
            "epoch  17 train_loss:  1.212641784123012 | train_acc:  0.6232220070702689\n",
            "        val_loss:  1.0848387479782104 | val_acc:  0.6667980551719666\n",
            "epoch  18 train_loss:  1.1739596026284354 | train_acc:  0.6377006326402936\n",
            "        val_loss:  1.050897479057312 | val_acc:  0.674759566783905\n",
            "epoch  19 train_loss:  1.1477480070931572 | train_acc:  0.6450788583074297\n",
            "        val_loss:  1.0166404247283936 | val_acc:  0.6830364465713501\n",
            "epoch  20 train_loss:  1.1128403084618705 | train_acc:  0.6581222585269383\n",
            "        val_loss:  0.9905036687850952 | val_acc:  0.691943883895874\n",
            "epoch  21 train_loss:  1.083936401775905 | train_acc:  0.6655152661459787\n",
            "        val_loss:  0.9673613905906677 | val_acc:  0.6982500553131104\n",
            "epoch  22 train_loss:  1.0609136274882724 | train_acc:  0.6752013393810817\n",
            "        val_loss:  0.9441425800323486 | val_acc:  0.7052656412124634\n",
            "epoch  23 train_loss:  1.037752410343715 | train_acc:  0.6816570401191712\n",
            "        val_loss:  0.9231204390525818 | val_acc:  0.7109411954879761\n",
            "epoch  24 train_loss:  1.02160279410226 | train_acc:  0.6876493079321725\n",
            "        val_loss:  0.9055219888687134 | val_acc:  0.7144884467124939\n",
            "epoch  25 train_loss:  1.0018479568617684 | train_acc:  0.6927549055644444\n",
            "        val_loss:  0.8914457559585571 | val_acc:  0.7196910381317139\n",
            "epoch  26 train_loss:  0.9812035935265677 | train_acc:  0.6986321755817958\n",
            "        val_loss:  0.8743588924407959 | val_acc:  0.7238688468933105\n",
            "epoch  27 train_loss:  0.9671466316495623 | train_acc:  0.7012840049607413\n",
            "        val_loss:  0.8626292943954468 | val_acc:  0.7267066240310669\n",
            "epoch  28 train_loss:  0.9528795838356018 | train_acc:  0.7068605286734445\n",
            "        val_loss:  0.8490886688232422 | val_acc:  0.730096161365509\n",
            "epoch  29 train_loss:  0.9405138151986259 | train_acc:  0.7093004294804164\n",
            "        val_loss:  0.8387066721916199 | val_acc:  0.7322245240211487\n",
            "epoch  30 train_loss:  0.9256429042134966 | train_acc:  0.7149812459945679\n",
            "        val_loss:  0.8272860050201416 | val_acc:  0.735062301158905\n",
            "epoch  31 train_loss:  0.9147852114268712 | train_acc:  0.718024514402662\n",
            "        val_loss:  0.8170843124389648 | val_acc:  0.7383730411529541\n",
            "epoch  32 train_loss:  0.9043964454105922 | train_acc:  0.7226201704570225\n",
            "        val_loss:  0.8098288774490356 | val_acc:  0.7386094927787781\n",
            "epoch  33 train_loss:  0.8900732721601213 | train_acc:  0.7254541431154523\n",
            "        val_loss:  0.7983675003051758 | val_acc:  0.7432603240013123\n",
            "epoch  34 train_loss:  0.8806251304490226 | train_acc:  0.729702307496752\n",
            "        val_loss:  0.7902617454528809 | val_acc:  0.747044026851654\n",
            "epoch  35 train_loss:  0.8705480933189392 | train_acc:  0.7316287023680551\n",
            "        val_loss:  0.7847983837127686 | val_acc:  0.7468075156211853\n",
            "epoch  36 train_loss:  0.863605557169233 | train_acc:  0.7343573621341161\n",
            "        val_loss:  0.7763891816139221 | val_acc:  0.7486993670463562\n",
            "epoch  37 train_loss:  0.8528807197298323 | train_acc:  0.7381295153072902\n",
            "        val_loss:  0.7670832872390747 | val_acc:  0.7520889639854431\n",
            "epoch  38 train_loss:  0.8480170369148254 | train_acc:  0.7381457907812936\n",
            "        val_loss:  0.7620989680290222 | val_acc:  0.7523254156112671\n",
            "epoch  39 train_loss:  0.8368146657943726 | train_acc:  0.7407059056418283\n",
            "        val_loss:  0.757375955581665 | val_acc:  0.7531925439834595\n",
            "epoch  40 train_loss:  0.8291603003229414 | train_acc:  0.7429872257368905\n",
            "        val_loss:  0.749450147151947 | val_acc:  0.7558726072311401\n",
            "epoch  41 train_loss:  0.8206161345754351 | train_acc:  0.7466415115765163\n",
            "        val_loss:  0.7438411116600037 | val_acc:  0.7568973898887634\n",
            "epoch  42 train_loss:  0.8142088890075684 | train_acc:  0.7482228602681841\n",
            "        val_loss:  0.7376315593719482 | val_acc:  0.7595775127410889\n",
            "epoch  43 train_loss:  0.8076206769262041 | train_acc:  0.7506533196994236\n",
            "        val_loss:  0.7320961952209473 | val_acc:  0.7607598900794983\n",
            "epoch  44 train_loss:  0.7995915532112121 | train_acc:  0.7526195355824061\n",
            "        val_loss:  0.7261254191398621 | val_acc:  0.7632035613059998\n",
            "epoch  45 train_loss:  0.7911324024200439 | train_acc:  0.7551371233803885\n",
            "        val_loss:  0.7208008766174316 | val_acc:  0.7651742100715637\n",
            "epoch  46 train_loss:  0.7890111480440413 | train_acc:  0.7564416221209935\n",
            "        val_loss:  0.7176506519317627 | val_acc:  0.7666719555854797\n",
            "epoch  47 train_loss:  0.7829567585672651 | train_acc:  0.7573015553610666\n",
            "        val_loss:  0.7125164270401001 | val_acc:  0.7667507529258728\n",
            "epoch  48 train_loss:  0.7736483335494995 | train_acc:  0.7608558484486171\n",
            "        val_loss:  0.7078932523727417 | val_acc:  0.7692732214927673\n",
            "epoch  49 train_loss:  0.7671513557434082 | train_acc:  0.7617607695715768\n",
            "        val_loss:  0.7028120756149292 | val_acc:  0.7711650729179382\n",
            "epoch  50 train_loss:  0.762986627646855 | train_acc:  0.7645869782992771\n",
            "        val_loss:  0.6996608376502991 | val_acc:  0.7710074186325073\n",
            "epoch  51 train_loss:  0.7555917263031006 | train_acc:  0.7667632017816816\n",
            "        val_loss:  0.6956887245178223 | val_acc:  0.7714803814888\n",
            "epoch  52 train_loss:  0.7510909148624965 | train_acc:  0.7673403773988996\n",
            "        val_loss:  0.690597414970398 | val_acc:  0.7732934355735779\n",
            "epoch  53 train_loss:  0.7430058768817357 | train_acc:  0.7693944045475551\n",
            "        val_loss:  0.6875577569007874 | val_acc:  0.7740817070007324\n",
            "epoch  54 train_loss:  0.7392814755439758 | train_acc:  0.7704308986663818\n",
            "        val_loss:  0.6844123601913452 | val_acc:  0.7728204727172852\n",
            "epoch  55 train_loss:  0.7344426717076983 | train_acc:  0.7735135623386928\n",
            "        val_loss:  0.6778984069824219 | val_acc:  0.7770771384239197\n",
            "epoch  56 train_loss:  0.7252949901989528 | train_acc:  0.7764020698411124\n",
            "        val_loss:  0.6754546165466309 | val_acc:  0.7775501012802124\n",
            "epoch  57 train_loss:  0.7209564106804984 | train_acc:  0.7771238275936672\n",
            "        val_loss:  0.6702972054481506 | val_acc:  0.7799937129020691\n",
            "epoch  58 train_loss:  0.7179812669754029 | train_acc:  0.7786005939756121\n",
            "        val_loss:  0.6667981147766113 | val_acc:  0.7803090214729309\n",
            "epoch  59 train_loss:  0.7127386672156197 | train_acc:  0.7783226472990853\n",
            "        val_loss:  0.6645504236221313 | val_acc:  0.7797572612762451\n",
            "epoch  60 train_loss:  0.7086730139596121 | train_acc:  0.7819609454699925\n",
            "        val_loss:  0.6598259806632996 | val_acc:  0.7813337445259094\n",
            "epoch  61 train_loss:  0.7009615727833339 | train_acc:  0.7821402856281825\n",
            "        val_loss:  0.6592234373092651 | val_acc:  0.7814126014709473\n",
            "epoch  62 train_loss:  0.6970417124884469 | train_acc:  0.7852641940116882\n",
            "        val_loss:  0.6529355645179749 | val_acc:  0.7835409045219421\n",
            "epoch  63 train_loss:  0.6947414347103664 | train_acc:  0.7852750880377634\n",
            "        val_loss:  0.6503682732582092 | val_acc:  0.7825161814689636\n",
            "epoch  64 train_loss:  0.6917489375386919 | train_acc:  0.7870650853429522\n",
            "        val_loss:  0.6466742157936096 | val_acc:  0.7848809957504272\n",
            "epoch  65 train_loss:  0.6819748112133571 | train_acc:  0.788867826121194\n",
            "        val_loss:  0.6432498693466187 | val_acc:  0.7868516445159912\n",
            "epoch  66 train_loss:  0.679417816230229 | train_acc:  0.7896247216633387\n",
            "        val_loss:  0.6401727199554443 | val_acc:  0.7873246073722839\n",
            "epoch  67 train_loss:  0.6748846360615322 | train_acc:  0.7913307360240391\n",
            "        val_loss:  0.6381585597991943 | val_acc:  0.7880340814590454\n",
            "epoch  68 train_loss:  0.6710405911718096 | train_acc:  0.7924298984663827\n",
            "        val_loss:  0.6342918276786804 | val_acc:  0.7904776930809021\n",
            "epoch  69 train_loss:  0.6654986773218428 | train_acc:  0.7947732601846967\n",
            "        val_loss:  0.6341185569763184 | val_acc:  0.7892164587974548\n",
            "epoch  70 train_loss:  0.6631080048424857 | train_acc:  0.7952106663158962\n",
            "        val_loss:  0.6285865306854248 | val_acc:  0.7913448214530945\n",
            "epoch  71 train_loss:  0.6563921502658299 | train_acc:  0.7952244520187378\n",
            "        val_loss:  0.6254433989524841 | val_acc:  0.7920542359352112\n",
            "epoch  72 train_loss:  0.6537308624812534 | train_acc:  0.7980410524777004\n",
            "        val_loss:  0.6272243857383728 | val_acc:  0.7915024757385254\n",
            "epoch  73 train_loss:  0.6512948172433036 | train_acc:  0.7989262955529349\n",
            "        val_loss:  0.620107114315033 | val_acc:  0.793867290019989\n",
            "epoch  74 train_loss:  0.6468791007995606 | train_acc:  0.7993674959455218\n",
            "        val_loss:  0.6180923581123352 | val_acc:  0.7947343587875366\n",
            "epoch  75 train_loss:  0.6403379610606602 | train_acc:  0.8026446495737348\n",
            "        val_loss:  0.6156600117683411 | val_acc:  0.7953649759292603\n",
            "epoch  76 train_loss:  0.6378172942570277 | train_acc:  0.8018696989331927\n",
            "        val_loss:  0.6121742129325867 | val_acc:  0.7966262102127075\n",
            "epoch  77 train_loss:  0.6334563187190465 | train_acc:  0.8051652772086008\n",
            "        val_loss:  0.6096919775009155 | val_acc:  0.7967050671577454\n",
            "epoch  78 train_loss:  0.627268648147583 | train_acc:  0.8054672598838806\n",
            "        val_loss:  0.6080745458602905 | val_acc:  0.7986757159233093\n",
            "epoch  79 train_loss:  0.6267231907163348 | train_acc:  0.8061943514006479\n",
            "        val_loss:  0.6042056679725647 | val_acc:  0.7999369502067566\n",
            "epoch  80 train_loss:  0.6201124157224382 | train_acc:  0.8095724276133947\n",
            "        val_loss:  0.6025952100753784 | val_acc:  0.8000158071517944\n",
            "epoch  81 train_loss:  0.619438978603908 | train_acc:  0.80812976871218\n",
            "        val_loss:  0.5999916195869446 | val_acc:  0.8005675673484802\n",
            "epoch  82 train_loss:  0.6139153684888567 | train_acc:  0.8092809694153922\n",
            "        val_loss:  0.5996199250221252 | val_acc:  0.8005675673484802\n",
            "epoch  83 train_loss:  0.6097492439406259 | train_acc:  0.8125066620962961\n",
            "        val_loss:  0.596045196056366 | val_acc:  0.8021441102027893\n",
            "epoch  84 train_loss:  0.6057699186461313 | train_acc:  0.8131864700998579\n",
            "        val_loss:  0.592512845993042 | val_acc:  0.8051395416259766\n",
            "epoch  85 train_loss:  0.6044409683772496 | train_acc:  0.8148003901754107\n",
            "        val_loss:  0.591359555721283 | val_acc:  0.802617073059082\n",
            "epoch  86 train_loss:  0.5996892298970904 | train_acc:  0.8152761237961905\n",
            "        val_loss:  0.5919680595397949 | val_acc:  0.8044301271438599\n",
            "epoch  87 train_loss:  0.5980484962463379 | train_acc:  0.8159063594681876\n",
            "        val_loss:  0.5865569710731506 | val_acc:  0.8037995100021362\n",
            "epoch  88 train_loss:  0.5934711354119437 | train_acc:  0.8172447000231061\n",
            "        val_loss:  0.5841956734657288 | val_acc:  0.8050607442855835\n",
            "epoch  89 train_loss:  0.5885132329804557 | train_acc:  0.8185572641236442\n",
            "        val_loss:  0.5817644000053406 | val_acc:  0.8071890473365784\n",
            "epoch  90 train_loss:  0.5865442701748439 | train_acc:  0.8188345381191798\n",
            "        val_loss:  0.5785895586013794 | val_acc:  0.8064796328544617\n",
            "epoch  91 train_loss:  0.5811429006712777 | train_acc:  0.8216105478150504\n",
            "        val_loss:  0.5774437785148621 | val_acc:  0.808056116104126\n",
            "epoch  92 train_loss:  0.5772108674049378 | train_acc:  0.8220925416265216\n",
            "        val_loss:  0.574805736541748 | val_acc:  0.8091596961021423\n",
            "epoch  93 train_loss:  0.5751771654401506 | train_acc:  0.8231243644441877\n",
            "        val_loss:  0.5742601156234741 | val_acc:  0.8096326589584351\n",
            "epoch  94 train_loss:  0.5682144250188556 | train_acc:  0.8247674533299038\n",
            "        val_loss:  0.5712085962295532 | val_acc:  0.8085290789604187\n",
            "epoch  95 train_loss:  0.5670612726892744 | train_acc:  0.8252876452037267\n",
            "        val_loss:  0.5704715251922607 | val_acc:  0.8099479675292969\n",
            "epoch  96 train_loss:  0.5649513619286674 | train_acc:  0.8256236348833357\n",
            "        val_loss:  0.5673694014549255 | val_acc:  0.8108938932418823\n",
            "epoch  97 train_loss:  0.5622267127037048 | train_acc:  0.8278942023004804\n",
            "        val_loss:  0.5663877129554749 | val_acc:  0.8118398189544678\n",
            "epoch  98 train_loss:  0.5562465463365828 | train_acc:  0.8284759129796709\n",
            "        val_loss:  0.5644944906234741 | val_acc:  0.8123916387557983\n",
            "epoch  99 train_loss:  0.5566183754376003 | train_acc:  0.8290646808488028\n",
            "        val_loss:  0.5617021322250366 | val_acc:  0.8123127818107605\n",
            "epoch  100 train_loss:  0.5527403695242745 | train_acc:  0.8298637798854283\n",
            "        val_loss:  0.559070348739624 | val_acc:  0.8141258358955383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWc2mBzjyol5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "888d47b5-f144-47a9-9eff-107400ff0260"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "range_list = [i for i in range(len(train_loss_list))] \n",
        "plt.plot(range_list, train_loss_list, label = \"train loss\")\n",
        "plt.plot(range_list, val_loss_list, label = \"val loss\")\n",
        "plt.xlabel('iter')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MForbqKZE9kX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925bc6b5-1af1-435b-fcc4-8b00c334f119"
      },
      "source": [
        "test_dataset = POS_Dataset(test_sentence, test_pos, word2idx, dict_pos)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, collate_fn=pad_collate)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "def idx2str(idx_list, idx_dict):\n",
        "  temp_str = []\n",
        "  for idx in idx_list:\n",
        "    temp_str.append(idx_dict[idx.item()])\n",
        "  \n",
        "  return temp_str\n",
        "\n",
        "for (x_padded, y_padded, x_lens, y_lens) in test_loader:\n",
        "  inputs, targets = x_padded.to(device), y_padded.to(device)\n",
        "  print(idx2str(inputs[0], idx2word))\n",
        "  print(idx2str(targets[0], idx2pos))\n",
        "  outputs = model(inputs, x_lens).view(-1, len(dict_pos)).argmax(dim = -1)\n",
        "  temp_str = []\n",
        "  for idx in outputs:\n",
        "    temp_str.append(idx2pos[idx.item()])\n",
        "  print(temp_str)\n",
        "  print(\"-----------------------\")\n",
        "  \n",
        "  break\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['此線', '設', '12', '歲', '以下', '小童', '及', '65', '歲', '或', '以上', '長者', '半', '價', '優惠', '，', '此線', '所有', '巴士', '均', '接受', '八', '達', '通', '卡', '付款', '，', '上車', '付款', '、', '不', '設', '找續', '。']\n",
            "['NOUN', 'VERB', 'NUM', 'NOUN', 'ADP', 'NOUN', 'CCONJ', 'NUM', 'NOUN', 'CCONJ', 'DET', 'NOUN', 'NUM', 'NOUN', 'NOUN', 'PUNCT', 'NOUN', 'DET', 'NOUN', 'ADV', 'VERB', 'NUM', 'VERB', 'PART', 'PART', 'VERB', 'PUNCT', 'VERB', 'VERB', 'PUNCT', 'ADV', 'VERB', 'VERB', 'PUNCT']\n",
            "['NOUN', 'VERB', 'NUM', 'NOUN', 'NUM', 'NOUN', 'CCONJ', 'PART', 'NOUN', 'CCONJ', 'ADP', 'NOUN', 'NUM', 'NOUN', 'VERB', 'PUNCT', 'NOUN', 'DET', 'NOUN', 'ADV', 'VERB', 'NUM', 'VERB', 'PART', 'PART', 'NOUN', 'PUNCT', 'NOUN', 'NOUN', 'PUNCT', 'ADV', 'VERB', 'NOUN', 'PUNCT']\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncKVYNIwIJuD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}